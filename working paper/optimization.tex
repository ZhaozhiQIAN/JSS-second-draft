%% LyX 2.2.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{mathptmx}
\renewcommand{\familydefault}{\rmdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{amsbsy}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{hyperref}
\usepackage{natbib}
\setcitestyle{round}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatother

\usepackage{babel}
\begin{document}

\subsection{Parameter estimation}

\subsubsection{Estimating central ranking $\pi_{0}$}

Given a set of weights $w$ and $n$ observed rankings $\sigma_{1},\ldots,\sigma_{n}$,
finding the maximum likelihood estimate of central ranking $\pi_{0}$
is equivalent to minimize the total distance between $\pi_{0}$ and
the observed rankings \ref{eq:rank-agg-obj}.
\begin{equation}
\pi_{0}=\argmin\left\{ \sum_{i=1}^{n}D_{wK}(\pi_{0},\sigma_{i})\right\} \label{eq:rank-agg-obj}
\end{equation}
\cite{dwork2001rank} studied this problem where the distance metric
is the original Kendall distance, i.e. the weights are all equal.
They showed that even when $n=4$, the problem is NP-complete. We
have seen little evidence suggesting that introducing different weights
will reduce the complexity of this problem. Therefore, in most cases
the optimization algorithm will only be able to find a local optimal.
A ranking $\pi$ is a local optimal of problem \ref{eq:rank-agg-obj}
if there is no ranking $\pi'$ that can be obtained from $\pi$ by
performing a single adjacent transposition and $\sum_{i=1}^{n}D_{wK}(\pi,\sigma_{i})>\sum_{i=1}^{n}D_{wK}(\pi',\sigma_{i})$. 

\cite{dwork2001rank} showed that in the equal-weighting case all
the local optimals of problem \ref{eq:rank-agg-obj} have a desirable
property, the extended Condorcet property. The extended Condorcet
property requires that if there exists a partition of items $(C,\,C')$
such that for any $x\in C$ and $y\in C'$ the majority of $\sigma_{1},\ldots,\sigma_{n}$
prefers $x$ over $y$, then $x$ must be ranked above $y$. Roughly
speaking, the extended Condorcet property ensures that the ``consensus''
in the observed rankings are preserved in the aggregated ranking.
Not all rank aggregation algorithm has extended Condorcet property
and the Borda count algorithm is such an exception \cite{young1974axiomatization}. 

When the weights are different, the local optimals of problem \ref{eq:rank-agg-obj}
do not have extended Condorcet property either, but this should be
preceived as a feature of the weighted Kendall distance. Essentially,
the weighted Kendall distance captures the senario when the disagreement
at the top of the ranked list of objects matters more then the one
at the bottom. Hence, both the number of disagreements (consenses)
and the location of the disagreements will affect the weighted Kendall
aggregation while the extended Condorcet property does not take the
location into account. 

We propose the following huristic algorithm to find local optimals.
The algorithm is provided with an initial value of $\pi_{0}$, denoted
as $\pi_{00}$. This initial value could be a frequently observed
ranking or the result of another rank aggregation algorithm such as
the Borda count algorithm. Then all its neighboring rankings are considered
and their log-likelihoods are calculated. If $\pi_{00}$ achieves
the largest likelihood among all its neighbouts then we have found
the local optimum and the algorithm stops. Otherwise, we select the
one with the largest likelihood as $\pi_{01}$ and consider all of
its neighbors again. This process repeats until $\pi_{0k}$ has larger
likelihood than all its neighbors. Since in each step the objective
function \ref{eq:rank-agg-obj} always decreases and there are only
a limited number of $\pi_{0}$'s to choose from, the algorithm will
stop eventually. In section we evaluate this huristic algorithm with
simulation studies. The simulation result shows that when the observed
rankings are generated by weighted Kendall model, the algorithm is
likely to find the true ranking even when the number of objects are
large (\textasciitilde{}40) and the sample size is small (\textasciitilde{}200). 

\subsubsection{Estimating weights $w$}

The weights in the weighted Kendall distance is closely related to
the 'dispersion' of rankings. They also indicate the relative importance
of locations in the ranked list. Hence, the estimation of weights
is an important part of the model.

For the weighted Kendall model the log-likelihood function is given
by:

\[
\ell(\pi_{1},\pi_{2},...,\pi_{n})=-\sum_{i=1}^{n}\sum_{j=1}^{t-1}w_{j}Q_{j}(\pi_{i},\pi_{0})-n\log(C_{wK}(\boldsymbol{w})),
\]
where $Q_{j}(\pi_{i},\pi_{0})$ are defined as in (\ref{eq:3}), and
they are constants if $\pi_{0}$ is given. In section  we've shown
that the logrithm of the normalizing constant $C_{wK}(\boldsymbol{w})$
is a convex function in $w$. It follows that the log-likelihood function
is a concave function in $w$. Since the non-decreasing constraint
on $w$ is a linear inequality constraint, the problem of finding
maximum likelihood estimate of $w$ is a convex optimization problem,
which has a global optimal solution. 

In practice, we reparametrize $w_{j}$ as $w_{j}=\sum_{i=j}^{t-1}\phi_{i}$,
where $\phi_{i}\geq0$ for all $i$, and transform the non-increasing
constraint on $w$ into a box constraint on $\phi$. The log-likelihood
function is still concave and it can be rewritten in terms of $\phi$
as:
\[
\ell(\pi_{1},\pi_{2},...,\pi_{n})=-\sum_{i=1}^{n}\sum_{j=1}^{t-1}\phi_{j}\left[\sum_{k=j}^{t-1}Q_{k}(\pi_{i},\pi_{0})\right]-n\log(C_{wK}(\boldsymbol{w}(\boldsymbol{\phi}))).
\]

With the simplified constraint, we can apply a general convex optimization
solver to estimate $w$. The L-BFGS-B method in the \proglang{R}
package \pkg{Optimx} is used for the optimization in \pkg{rankdist}.
The simulation studies in section  confirms that the optimization
procedure is reliable. 

\subsubsection{Estimating central ranking $\pi_{0}$ and weights $w$ jointly}

In most cases, neither $\pi_{0}$ nor $w$ is known to us and both
need to be estimated from the data. It is hard to estimate $\pi_{0}$
and $w$ simuntanously because $\pi_{0}$ is a ranking while $w$
is a vector of real numbers. Instead, we apply a stage-wise method
that iteratively searches for the two paramters. 

The algorithm is provided with an initial value of $\pi_{0}$, denoted
as $\pi_{00}$, which can be the result of any rank aggregation algorithm.
The algorithm finds the optimal weights for $\pi_{00}$, denoted as
$w_{0}$ and records the resulting log-likelihood value. Then all
the neighboring rankings of $\pi_{00}$ are considered. For each of
them, the algorithm finds the optimal weights $w$ and calculate the
resulting log-likelihood value. If $\pi_{00}$ and $w_{0}$ achieve
the highest likelihood among all neighbours then the algorithm stops
and returns $\pi_{00}$ and $w_{0}$. Otherwise, the algorithm selects
the neighbour with the largest likelihood as $\pi_{01}$ and consider
all of its neighbors again. This process repeats until $\pi_{0k}$
and $w_{k}$ has larger likelihood than all neighbors of $\pi_{0k}$.
The algorithm will stop eventually because there are only a limited
number of candidate central rankings. 

We evaluate the joint optimization algorithm in section  with simulation
studies. The simulation result shows that when the observed rankings
are generated by weighted Kendall model, the algorithm is likely to
find the true ranking and the true weights even when the number of
objects are large (\textasciitilde{}40) and the sample size is reletively
small (\textasciitilde{}500). 

\bibliographystyle{apalike}
\bibliography{paper_ref}

\end{document}
