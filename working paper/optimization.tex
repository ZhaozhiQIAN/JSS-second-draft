%% LyX 2.2.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{mathptmx}
\renewcommand{\familydefault}{\rmdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{array}
\usepackage{float}
\usepackage{multirow}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{hyperref}
\usepackage{natbib}
\setcitestyle{round}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatother

\usepackage{babel}
\begin{document}

\subsection{Parameter estimation}

\subsubsection{Estimating central ranking $\pi_{0}$\label{subsec:Estimating-central-ranking}}

Given a set of weights $w$ and $n$ observed rankings $\sigma_{1},\ldots,\sigma_{n}$,
finding the maximum likelihood estimate of central ranking $\pi_{0}$
is equivalent to minimize the total distance between $\pi_{0}$ and
the observed rankings \ref{eq:rank-agg-obj}.
\begin{equation}
\pi_{0}=\argmin\left\{ \sum_{i=1}^{n}D_{wK}(\pi_{0},\sigma_{i})\right\} \label{eq:rank-agg-obj}
\end{equation}
\citet{dwork2001rank} studied this problem where the distance metric
is the original Kendall distance, i.e. the weights are all equal.
They showed that even when $n=4$, the problem is NP-complete. We
have seen little evidence suggesting that introducing different weights
will reduce the complexity of this problem. Therefore, in most cases
the optimization algorithm will only be able to find a local optimal.
A ranking $\pi$ is a local optimal of problem \ref{eq:rank-agg-obj}
if there is no ranking $\pi'$ that can be obtained from $\pi$ by
performing a single adjacent transposition and $\sum_{i=1}^{n}D_{wK}(\pi,\sigma_{i})>\sum_{i=1}^{n}D_{wK}(\pi',\sigma_{i})$. 

\citet{dwork2001rank} showed that in the equal-weighting case all
the local optimals of problem \ref{eq:rank-agg-obj} have a desirable
property, the extended Condorcet property. The extended Condorcet
property requires that if there exists a partition of items $(C,\,C')$
such that for any $x\in C$ and $y\in C'$ the majority of $\sigma_{1},\ldots,\sigma_{n}$
prefers $x$ over $y$, then $x$ must be ranked above $y$. Roughly
speaking, the extended Condorcet property ensures that the ``consensus''
in the observed rankings are preserved in the aggregated ranking.
Not all rank aggregation algorithm has extended Condorcet property
and the Borda count algorithm is such an exception \citep{young1974axiomatization}. 

When the weights are different, the local optimals of problem \ref{eq:rank-agg-obj}
do not have extended Condorcet property either, but this should be
preceived as a feature of the weighted Kendall distance. Essentially,
the weighted Kendall distance captures the senario when the disagreement
at the top of the ranked list of objects matters more then the one
at the bottom. Hence, both the number of disagreements (consenses)
and the location of the disagreements will affect the weighted Kendall
aggregation while the extended Condorcet property does not take the
location into account. 

We propose the following huristic algorithm to find local optimals.
The algorithm is provided with an initial value of $\pi_{0}$, denoted
as $\pi_{00}$. This initial value could be a frequently observed
ranking or the result of another rank aggregation algorithm such as
the Borda count algorithm. Then all its neighboring rankings are considered
and their log-likelihoods are calculated. If $\pi_{00}$ achieves
the largest likelihood among all its neighbouts then we have found
the local optimum and the algorithm stops. Otherwise, we select the
one with the largest likelihood as $\pi_{01}$ and consider all of
its neighbors again. This process repeats until $\pi_{0k}$ has larger
likelihood than all its neighbors. Since in each step the objective
function \ref{eq:rank-agg-obj} always decreases and there are only
a limited number of $\pi_{0}$'s to choose from, the algorithm will
stop eventually. In section we evaluate this huristic algorithm with
simulation studies. The simulation result shows that when the observed
rankings are generated by weighted Kendall model, the algorithm is
likely to find the true ranking even when the number of objects are
large (\textasciitilde{}40) and the sample size is small (\textasciitilde{}200). 

\subsubsection{Estimating weights $w$\label{subsec:Estimating-weights}}

The weights in the weighted Kendall distance is closely related to
the 'dispersion' of rankings. They also indicate the relative importance
of locations in the ranked list. Hence, the estimation of weights
is an important part of the model.

For the weighted Kendall model the log-likelihood function is given
by:

\[
\ell(\pi_{1},\pi_{2},...,\pi_{n})=-\sum_{i=1}^{n}\sum_{j=1}^{t-1}w_{j}Q_{j}(\pi_{i},\pi_{0})-n\log(C_{wK}(\boldsymbol{w})),
\]
where $Q_{j}(\pi_{i},\pi_{0})$ are defined as in (\ref{eq:3}), and
they are constants if $\pi_{0}$ is given. In section  we've shown
that the logrithm of the normalizing constant $C_{wK}(\boldsymbol{w})$
is a convex function in $w$. It follows that the log-likelihood function
is a concave function in $w$. Since the non-decreasing constraint
on $w$ is a linear inequality constraint, the problem of finding
maximum likelihood estimate of $w$ is a convex optimization problem,
which has a global optimal solution. 

In practice, we reparametrize $w_{j}$ as $w_{j}=\sum_{i=j}^{t-1}\phi_{i}$,
where $\phi_{i}\geq0$ for all $i$, and transform the non-increasing
constraint on $w$ into a box constraint on $\phi$. The log-likelihood
function is still concave and it can be rewritten in terms of $\phi$
as:
\[
\ell(\pi_{1},\pi_{2},...,\pi_{n})=-\sum_{i=1}^{n}\sum_{j=1}^{t-1}\phi_{j}\left[\sum_{k=j}^{t-1}Q_{k}(\pi_{i},\pi_{0})\right]-n\log(C_{wK}(\boldsymbol{w}(\boldsymbol{\phi}))).
\]

With the simplified constraint, we can apply a general convex optimization
solver to estimate $w$. The L-BFGS-B method in the \proglang{R}
package \pkg{Optimx} is used for the optimization in \pkg{rankdist}.
The simulation studies in section  confirms that the optimization
procedure is reliable. 

\subsubsection{Estimating central ranking $\pi_{0}$ and weights $w$ jointly\label{subsec:Estimating-both}}

In most cases, neither $\pi_{0}$ nor $w$ is known to us and both
need to be estimated from the data. It is hard to estimate $\pi_{0}$
and $w$ simuntanously because $\pi_{0}$ is a ranking while $w$
is a vector of real numbers. Instead, we apply a stage-wise method
that iteratively searches for the two paramters. 

The algorithm is provided with an initial value of $\pi_{0}$, denoted
as $\pi_{00}$, which can be the result of any rank aggregation algorithm.
The algorithm finds the optimal weights for $\pi_{00}$, denoted as
$w_{0}$ and records the resulting log-likelihood value. Then all
the neighboring rankings of $\pi_{00}$ are considered. For each of
them, the algorithm finds the optimal weights $w$ and calculate the
resulting log-likelihood value. If $\pi_{00}$ and $w_{0}$ achieve
the highest likelihood among all neighbours then the algorithm stops
and returns $\pi_{00}$ and $w_{0}$. Otherwise, the algorithm selects
the neighbour with the largest likelihood as $\pi_{01}$ and consider
all of its neighbors again. This process repeats until $\pi_{0k}$
and $w_{k}$ has larger likelihood than all neighbors of $\pi_{0k}$.
The algorithm will stop eventually because there are only a limited
number of candidate central rankings. 

We evaluate the joint optimization algorithm in section  with simulation
studies. The simulation result shows that when the observed rankings
are generated by weighted Kendall model, the algorithm is likely to
find the true ranking and the true weights even when the number of
objects are large (\textasciitilde{}40) and the sample size is reletively
small (\textasciitilde{}500). 

\subsection[Simulation study]{Simulation study}

The aim of the simulation study is to verify that the estimation procedure
produces reasonable parameter estimations when the sample rankings
are generated by the weighted Kendall model. We are especially interested
in cases when the number of objects $t$ is large (more than 20).
For simulations of large $t$, the first problem is to generate samples
from the pre-defined weighted Kendall distribution. The naive way
is to calculate the probability of each ranking and take samples from
the corresponding categorical distribution. The naive method does
not scale up as it involves calculating and storing $t!$ probabilities.
The typical alternatives are MCMC type algorithms such as the Metropolice-Hasting
algorithm. These algorithms are computational and memory efficient
but the generated samples will not be independent. In the next section,
we introduce a way to efficiently obtain independent samples from
weighted Kendall model. Then we proceed to the simulation results.

\subsubsection[Generating samples]{ Generating samples from weighted Kendall model}

A sample ranking $\sigma$ can be recursively generated from weighted
Kendall model with $\pi_{0}$ and $\boldsymbol{w}$. In the first
stage, the algorithm samples the item to be ranked first i.e. $\sigma^{-1}(1)$
from $t$ available items. The probability for choosing item $k$
as the first item is given by
\[
\Prob\left(\sigma^{-1}(1)=k\right)\propto\exp\left(-\sum_{i=1}^{\pi_{0}(k)-1}w_{i}\right)
\]
The summation inside represents the weights accumulated when item
$k$ is moved to the top of $\pi_{0}$ via adjacent transpositions. 

After $\sigma^{-1}(1)$ is sampled, the item will be removed from
$\pi_{0}$. The weights should also be updated by removing $w_{1}$
and keeping the remaining ones. The same procedure is the used again
with updated $\pi_{0}$ and $\boldsymbol{w}$. The algorithm chooses
one item from the remaining $t-1$ items to be $\sigma^{-1}(2)$. 

After $t$ steps, $\sigma$ will be a complete ranking and the recursion
will end. Hence the complexity of obtaining one sample is $O(t^{2})$,
a big improvement over the naive method.

\subsubsection[Estimating pi0 with given w]{ Estimating $\pi_{0}$ with given $w$\label{subsec:-Estimating-pi0-sim}}

This simulation study tests the heuristic algorithm presented in section
\ref{subsec:Estimating-central-ranking}. A data set of $n$ sample
rankings of $t$ objects is generated from weighted Kendall model
with a certain $\pi_{0}$ and $\boldsymbol{w}$. We choose $t$ to
be 40 to capture the large-$t$ senario and test the scalibility of
the algorithm. The choice of $\pi_{0}$ does not affect the result
of this simulation because the distance metric is invariant under
relabeling of items. The weight $\boldsymbol{w}$ is decreasing and
$w_{i}=\log(41-i)/5$. The weight gives the right amount of dispersion
so that the sample rankings are highly likely to be unique.

The algrorithm is provided with the sample data, the true weights
$\boldsymbol{w}$, and an initial ranking $\pi_{00}$ estimated using
the Borda count method. We record the Kendall distance between the
central ranking found by the algorithm and the true $\pi_{0}$. This
precedure is then repeated for $500$ times. In table \ref{tab:sim1},we
tabulate the distribution of distance for two simulations with sample
sizes $n=200$ and $500$ respectively. The distance between the initial
ranking (Borda count method) and $\pi_{0}$ is also shown for reference.
We observe that in general the rankings produced by the heuristic
algorithm is closer to the true $\pi_{0}$ than the initial ranking.

\begin{table}[H]
\begin{centering}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
 & Distance & 0 & 1 & 2 & 3\tabularnewline
\hline 
\hline 
\multirow{2}{*}{n=200} & initial & 263 & 190 & 42 & 5\tabularnewline
\cline{2-6} 
 & final & 354 & 128 & 16 & 2\tabularnewline
\hline 
\multirow{2}{*}{n=500} & initial & 422 & 76 & 2 & 0\tabularnewline
\cline{2-6} 
 & final & 450 & 49 & 1 & 0\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Distribution of Kendall distance between the true $\pi_{0}$ and the
estimated ones. The results for two simulations with sample sizes
200 and 500. Type 'initial' represents the distance between the initial
ranking (Borda count method) and true $\pi_{0}$, and type 'final'
represents the distance between the ranking estimated by the algorithm
and true $\pi_{0}$. The number in the cell represents the number
of runs that the distance is observed. The numbers in each row sum
up to 500 because the sampling-estimation procedure is repeated for
500 times for each simulation.\label{tab:sim1}}
\end{table}


\subsubsection[Estimating w with given pi0]{ Estimating $w$ with given $\pi_{0}$\label{subsec:-Estimating-w-sim}}

This simulation study tests the algorithm for estimating weights $\boldsymbol{w}$
presented in section \ref{subsec:Estimating-weights}. The data generation
procedure is the same as section \ref{subsec:-Estimating-pi0-sim}.
The choice of $\pi_{0}$ and $\boldsymbol{w}$ also remains the same
but the sample size is increased to be 50000.

The weight estimation algorithm is provided with the sample data and
the true ranking $\pi_{0}$. We record the estimated weights in each
run. This precedure is then repeated for $500$ times. Figure \ref{fig:sim-2-weights}
illustrates the true weights $w_{i}$ (red dots) and the distribution
of estimated weights in the 500 runs (boxplot). In most cases the
true weight is very close to the median value of the estimated weights.
In all cases, the true weights fall between the quartiles of estimated
weights.

\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.3]{\string"Estimating weights\string".png}
\par\end{centering}
\caption{\label{fig:sim-2-weights}Visualization of true weights and estimated
weights. The simulation involves 39 weight parameters, which are plotted
across the horizontal axis. The true weight is represented as a red
dot. The distribution of the estimated weight in the 500 runs is illustrated
via a boxplot. The boxplot encodes the median, lower and upper quartiles,
and outliers.}
\end{figure}


\subsubsection[Estimating both pi0 and w]{ Estimating both $\pi_{0}$ and $w$}

This simulation study tests the algorithm that jointly estimates $\pi_{0}$
and $w$ presented in section \ref{subsec:Estimating-both}. The data
generation procedure is the same as section \ref{subsec:-Estimating-pi0-sim}.
The choice of $\pi_{0}$ and $\boldsymbol{w}$ also remains the same
and the sample size is chosen to be 500.

The algrorithm is provided with the sample data and an initial ranking
$\pi_{00}$ estimated using the Borda count method. We record the
Kendall distance between the central ranking found by the algorithm
and the true $\pi_{0}$ as well as the estimated weights. This precedure
is then repeated for $500$ times. 

In table \ref{tab:sim3}, we tabulate the distribution of recorded
distance. The distance between the initial ranking (Borda count method)
and $\pi_{0}$ is also shown for reference. We observe that in general
the rankings estimated by the algorithm is closer to the true $\pi_{0}$
than the initial ranking. Compared with the results in \ref{subsec:-Estimating-pi0-sim},
the estimation of $\pi_{0}$ does not seem to be significantly worse
when weights are also unknown. 

Figure \ref{fig:sim-3-weights} illustrates the true weights $w_{i}$
(red dots) and the distribution of estimated weights in the 500 runs
(boxplot). Similar to the results in section \ref{subsec:-Estimating-w-sim}
we found that in most cases the true weight is very close to the median
value of the estimated weights. In all cases, the true weights fall
between the quartiles of estimated weights.

This simulation suggests that $\pi_{0}$ and $w$ can be estimated
reliably by the algorithm.

\begin{table}[h]
\begin{centering}
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & Distance & 0 & 1 & 2\tabularnewline
\hline 
\hline 
\multirow{2}{*}{n=500} & initial & 419 & 88 & 1\tabularnewline
\cline{2-5} 
 & final & 453 & 47 & 0\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Distribution of Kendall distance between the true $\pi_{0}$ and the
estimated ones. The sample size for this simulation is 500. Type 'initial'
represents the distance between the initial ranking (Borda count method)
and true $\pi_{0}$, and type 'final' represents the distance between
the ranking estimated by the algorithm and true $\pi_{0}$. The number
in the cell represents the number of runs that the distance is observed.
The numbers in each row sum up to 500 because the sampling-estimation
procedure is repeated for 500 times.\label{tab:sim3}}
\end{table}

\begin{figure}[h]
\begin{centering}
\includegraphics[scale=0.3]{\string"Estimating weights full\string".png}
\par\end{centering}
\caption{\label{fig:sim-3-weights}Visualization of true weights and estimated
weights. The simulation involves 39 weight parameters, which are plotted
across the horizontal axis. The true weight is represented as a red
dot. The distribution of the estimated weight in the 500 runs is illustrated
via a boxplot. The boxplot encodes the median, lower and upper quartiles,
and outliers.}
\end{figure}

\bibliographystyle{apalike}
\bibliography{paper_ref}

\end{document}
